{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Index","text":""},{"location":"#index","title":"Index","text":"<pre><code>mkdocs.yml\ndocs/\n    overview.md\n    installation.md\n    building_docker_images.md\n    command_line_interface.md\n    example_commands.md\n    collecting_results.md\n    live_monitoring.md\n    considerations_on_accuracy.md\n    build_docs.md\n    work_to_do.md\n</code></pre>"},{"location":"build_docs/","title":"Build Documentation","text":""},{"location":"build_docs/#build-documentation","title":"Build Documentation","text":""},{"location":"build_docs/#build-and-run-the-docs-docker-container","title":"Build and Run the Docs Docker Container","text":"<p>To start the Docs container in the background, use the following command:</p> <pre><code>cd iris-gpubench/docs\n./build.sh\n</code></pre> <p>Once running, you can access the documentation at: http://localhost:8000/</p>"},{"location":"build_docs/#stop-and-remove-the-docs-docker-container","title":"Stop and Remove the Docs Docker Container","text":"<p>To stop and remove the Docs container, execute:</p> <pre><code>./stop_and_remove.sh\n</code></pre> <p>Previous Page | Next Page</p>"},{"location":"building_docker_images/","title":"Building Benchmark Docker Images","text":"<p>If you need to build Docker images for benchmarking, you can use the provided <code>build_images.sh</code> script. This script will build images defined in the <code>Benchmark_Docker</code> directory. Here\u2019s how to use it:</p> <p>1.Navigate to the Docker Directory:    Go to the <code>Benchmark_Docker</code> directory:</p> <pre><code>cd Benchmark_Docker\n</code></pre> <p>2.Run the Build Script:    Execute the build script to build all Docker images:</p> <pre><code>./build_images.sh\n</code></pre> <p>This script will build Docker images from the Dockerfiles located in <code>Benchmark_Docker/Benchmark_Dockerfiles</code>. Feel Free to add your own. The available Dockerfiles and their purposes are:</p> <ul> <li> <p>Base Images:</p> <ul> <li><code>Dockerfile.gpu_base</code>: Base image for GPU-based benchmarks.</li> <li><code>Dockerfile.sciml_base</code>: Base image for SciML benchmarks.</li> <li><code>Dockerfile.mantid_base</code>: Base image for Mantid imaging benchmarks.</li> </ul> </li> <li> <p>Mantid Imaging Benchmarks:</p> <ul> <li><code>Dockerfile.mantid_run_1</code>: Dockerfile for Mantid benchmark run 1GB.</li> <li><code>Dockerfile.mantid_run_8</code>: Dockerfile for Mantid benchmark run 8GB.</li> </ul> </li> <li> <p>SciML Benchmarks:      The docker files are run on a fork of the sciml-bench repo.</p> <ul> <li><code>Dockerfile.mnist_tf_keras</code>: Dockerfile for MNIST classification Benchmark using TensorFlow/Keras.</li> <li><code>Dockerfile.stemdl_classification_2gpu</code>: Dockerfile for STEMDL classification using 1 GPU</li> <li><code>Dockerfile.stemdl_classification_2gpu</code>: Dockerfile for STEMDL classification using 2 GPUs (There must be 2 available).</li> <li><code>Dockerfile.synthetic_regression</code>: Dockerfile for synthetic regression benchmarks using  the options <code>-b hidden_size 9000 -b epochs 10</code>. </li> </ul> <p>See sciml-bench docs for more information on these benchmarks and their options.</p> </li> <li> <p>Dummy Benchmark Container:</p> <ul> <li><code>Dockerfile.dummy</code>: Designed for testing purposes, this Dockerfile sets up a container that runs for 5 minutes before terminating. It is primarily used to profile the GPU resource usage of this monitoring tool. Ideally, the monitor should operate in isolation from the benchmarks to avoid interference. However, currently, the monitor runs in the background on the same VM as the benchmark containers, which poses scalability limitations. To address this in the future, Docker Compose could be used to manage multiple containers simultaneously, but this would require an SSH-based solution to monitor them from an external VM.</li> </ul> </li> </ul> <p>This setup will prepare the environment and Docker images required for running your benchmarks effectively.</p>"},{"location":"building_docker_images/#adding-new-benchmarks","title":"Adding New Benchmarks","text":"<p>To add new benchmarks, place their setup Dockerfiles in the <code>Benchmark_Docker/Benchmark_Dockerfiles</code> directory. Each Dockerfile should include an entry point that runs the benchmark, such as:</p> <pre><code>ENTRYPOINT [\"/bin/bash\", \"-c\", \"cd /root/mantid_imaging_cloud_bench &amp;&amp; conda activate mantidimaging &amp;&amp; ./run_8.sh\"]\n</code></pre> <p>Additionally, update <code>Benchmark_Docker/build_images.sh</code> to include your new image, or you can manually build the image using the following command:</p> <pre><code>docker build -t &lt;image_name&gt; -f path/to/Dockerfile.&lt;image_name&gt; .\n</code></pre> <p>With the <code>iris-gpubench</code> package installed and the Benchmark Docker images built, you can now monitor the benchmarks using <code>iris-gpubench</code>. For details on how to use it, refer to the next page Command-Line Arguments.</p> <p>Previous Page | Next Page</p>"},{"location":"collecting_results/","title":"Collecting Results","text":"<p>By default Results are saved to iris-gpubench/results.</p>"},{"location":"collecting_results/#1-formatted-results","title":"1. Formatted Results","text":"<ul> <li>formatted_metrics.txt : Formatted version of metrics.yml, see example below.</li> </ul> <pre><code>GPU and Carbon Performance Results\n\n+---------------------------------------+------------------------+\n| Metric                                | Value                  |\n+=======================================+========================+\n| Benchmark Image Name                  | synthetic_regression   |\n+---------------------------------------+------------------------+\n| Elapsed Monitor Time of Container (s) | 245.627                |\n+---------------------------------------+------------------------+\n| Total GPU Energy Consumed (kWh)       | 0.00993                |\n+---------------------------------------+------------------------+\n| Total GPU Carbon Emissions (gCO2)     | 1.4196                 |\n+---------------------------------------+------------------------+\n\n+---------------------------------------+-----------+\n| Metric                                |     Value |\n+=======================================+===========+\n| Elapsed Monitor Time of Container (s) | 245.627   |\n+---------------------------------------+-----------+\n| Total GPU Energy Consumed (kWh)       |   0.00993 |\n+---------------------------------------+-----------+\n| Total GPU Carbon Emissions (gCO2)     |   1.4196  |\n+---------------------------------------+-----------+\n\nCarbon Information\n\n+------------------------------------+---------------------+\n| Metric                             | Value               |\n+====================================+=====================+\n| Average Carbon Forecast (gCO2/kWh) | 143.0               |\n+------------------------------------+---------------------+\n| Carbon Forecast Start Time         | 2024-08-12 17:35:12 |\n+------------------------------------+---------------------+\n| Carbon Forecast End Time           | 2024-08-12 17:39:18 |\n+------------------------------------+---------------------+\n\nGPU Information\n\n+------------------------------------------------+------------------------------------+\n| Metric                                         | Value                              |\n+================================================+====================================+\n| GPU Name                                       | Tesla V100-PCIE-32GB               |\n+------------------------------------------------+------------------------------------+\n| Average GPU Util. (for &gt;0.00% GPU Util.) (%)   | 92.90476                           |\n+------------------------------------------------+------------------------------------+\n| Avg GPU Power (for &gt;0.00% GPU Util.) (W)       | 167.42962 (Power Limit: 250)       |\n+------------------------------------------------+------------------------------------+\n| Avg GPU Temperature (for &gt;0.00% GPU Util.) (C) | 52.33333                           |\n\n\n+------------------------------------------------+------------------------------------+\n| Avg GPU Memory (for &gt;0.00% GPU Util.) (MiB)    | 6500.00595 (Total Memory: 32768.0) |\n+------------------------------------------------+------------------------------------+\n</code></pre>"},{"location":"collecting_results/#2-gpu-metric-png-plots","title":"2. GPU Metric png Plots","text":"<ul> <li>metrics_plot.png: Time series plots for gpu utilization, power usage, temperature and Memory. See example below:</li> </ul>"},{"location":"collecting_results/#2-gpu-metric-grafana-plots-export_to_victoria-not-working","title":"2. GPU Metric Grafana Plots (--export_to_victoria) (NOT WORKING)","text":"<p>INSERT GRAFANA LINK HERE</p>"},{"location":"collecting_results/#3-result-metrics","title":"3. Result Metrics","text":"<ul> <li>metrics.yml: yml with the Benchmark Score and GPU Energy Performance results.</li> </ul> <p>Previous Page | Next Page</p>"},{"location":"command_line_interface/","title":"Command-Line Interface","text":"<pre><code>iris-gpubench [--interval INTERVAL] [--carbon_region CARBON_REGION] [--live_plot] [--export_to_victoria] [--benchmark_image BENCHMARK_IMAGE] [--monitor_benchmark_logs]\n</code></pre> <p>The following optional arguments are supported:</p> <ul> <li><code>--no_live_monitor</code>: Disable live monitoring of GPU metrics. Default is enabled.</li> <li><code>--interval &lt;seconds&gt;</code>: Set the interval for collecting GPU metrics. Default is <code>5</code> seconds.</li> <li><code>--carbon_region &lt;region&gt;</code>: Specify the carbon region for the National Grid ESO Regional Carbon Intensity API. Default is <code>\"South England\"</code>.</li> <li><code>--no_plot</code>: Disable plotting of GPU metrics. Default is enabled.</li> <li><code>--live_plot</code>: Enable live plotting of GPU metrics.</li> <li><code>--export_to_victoria</code>: Enable exporting of collected data to VictoriaMetrics.</li> <li><code>--benchmark_image &lt;image&gt;</code>: Docker container image to run as a benchmark (required).</li> <li><code>--monitor_benchmark_logs</code>: Enable monitoring of container logs in addition to GPU metrics.</li> </ul>"},{"location":"command_line_interface/#help-option","title":"Help Option","text":"<p>To display the help message with available options, run:</p> <pre><code>iris-gpubench --help\n</code></pre>"},{"location":"command_line_interface/#notes","title":"Notes:","text":"<ul> <li>The <code>--benchmark_image</code> argument is required for specifying the Docker container image.</li> <li>live gpu metrics monitoring and saving a final plot are enabled by default; use <code>--no_live_monitor</code> and <code>--no_plot</code> to disable them, respectively.</li> <li>To view the available carbon regions, use <code>--carbon_region \"\"</code> to get a list of all regions.</li> <li>To list available Docker images, use <code>--benchmark_image \"\"</code> for a list of images.</li> </ul> <p>For example, commands please see the next page.</p> <p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD Previous Page | Next Page ======= Previous Page | Next Page</p> <p>33a6dab38b006cea0efdb6d9f9ac432e6158285d</p>"},{"location":"considerations_on_accuracy/","title":"Considerations On Accuracy","text":""},{"location":"considerations_on_accuracy/#carbon-metrics-accuracy-limitations","title":"Carbon Metrics Accuracy Limitations","text":"<ul> <li>The Carbon Data is collected in real-time from the National Grid ESO Regional Carbon Intensity API.</li> <li>The Carbon Forecast Readings are updated every 30 minutes. The monitor records the index values at the start and end of each interval and calculates an average. Therefore, the accuracy may be limited for containers that run longer than 30 minutes, as the index can fluctuate significantly over time.</li> <li>The Carbon Forecast can vary based on factors such as weather, time of day/year, and energy demand, resulting in fluctuations in total carbon emissions from one run to the next. Therefore, it serves as a real-time estimate. For a broader perspective, you can multiply the total energy by the average Carbon Emission Rate in the UK, which was 162 gCO2/kWh in 2023.</li> </ul>"},{"location":"considerations_on_accuracy/#gpu-metrics-accuracy-limitions","title":"GPU Metrics Accuracy Limitions","text":"<ul> <li>The GPU metrics come from pynvml which is a python interface for NVML and \"nvidia-smi\" results.</li> <li>The \"error in nvidia-smi's power draw is \u00b1 5%\".</li> <li>Total energy is calculated by integrating power readings over time using the trapezoidal integration method. The accuracy of this calculation depends on the monitoring interval: a smaller interval results in more accurate energy estimates.</li> </ul>"},{"location":"considerations_on_accuracy/#profiling-the-monitors-impact-on-gpu-resources","title":"Profiling the Monitors Impact on GPU Resources","text":"<ul> <li>(Minimal) GPU Resource Usage by the Monitor: The monitoring tool consumes a small portion of GPU resources. For instance, a ~5-minute test with a dummy container shows negligible GPU usage, see below. CPU resources are also utilized, though profiling tests to determine exact CPU usage have not yet been conducted.</li> </ul> <pre><code>GPU and Carbon Performance Results\n\n+---------------------------------------+---------+\n| Metric                                | Value   |\n+=======================================+=========+\n| Benchmark Image Name                  | dummy   |\n+---------------------------------------+---------+\n| Elapsed Monitor Time of Container (s) | 320.177 |\n+---------------------------------------+---------+\n| Total GPU Energy Consumed (kWh)       | 0.00183 |\n+---------------------------------------+---------+\n| Total GPU Carbon Emissions (gCO2)     | 0.36118 |\n+---------------------------------------+---------+\n</code></pre> <ul> <li>These are idle usage levels, so monitoring the GPUs has a negligible impact on GPU resources.</li> </ul> <p>Previous Page | Next Page</p>"},{"location":"example_commands/","title":"Example Commands","text":"<ol> <li>Basic Monitoring with Completion Plot:</li> </ol> <pre><code>iris-gpubench --benchmark_image \"synthetic_regression\"\n</code></pre> <ol> <li>Exporting Data to VictoriaMetrics:</li> </ol> <pre><code>iris-gpubench --benchmark_image \"synthetic_regression\" --export_to_victoria\n</code></pre> <ol> <li>Full Command with All Options:</li> </ol> <pre><code>iris-gpubench --benchmark_image \"synthetic_regression\" --interval 10 --carbon_region \"South England\" --live_plot --export_to_victoria --monitor_benchmark_logs\n</code></pre> <p>Previous Page | Next Page</p>"},{"location":"installation/","title":"Installation","text":"<p>To set up the project, follow these steps:</p>"},{"location":"installation/#installation-instructions","title":"Installation Instructions","text":"<p>Start with an Ubuntu VM equipped with Nvidia GPUs and ensure the following dependencies are pre-installed: Docker, Nvidia Docker (for GPU support), Python with Virtual Environment and Nvidia drivers. If these are not installed, you can run ./setup_vm.sh from the cloned Git repository to set them up automatically. (Note: Git is required to clone the repository\u2014see step 1).</p> <p>Follow these steps to set up the project:</p> <p>1.Clone the Repository    Start by cloning the project repository:</p> <pre><code>git clone https://github.com/bryceshirley/iris-gpubench.git\ncd iris-gpubench\n</code></pre> <p>2.Set Up a Virtual Environment    Next, create and activate a virtual environment:</p> <pre><code>python3 -m venv env\nsource env/bin/activate\n</code></pre> <p>3.Install Dependencies and iris-gpubench Package    a. Finally, install the package along with necessary dependencies:</p> <pre><code>pip install wheel\npip install .\n</code></pre> <p>b. (For Developers)</p> <pre><code>pip install wheel\npip install -e .\n</code></pre> <ul> <li><code>-e</code> for editable mode, lets you install Python packages in a way that    allows immediate reflection of any changes you make to the source code    without needing to reinstall the package.</li> </ul> <p>4.Next Build Docker Images for the Benchmarks</p> <p>Previous Page | Next Page</p>"},{"location":"live_monitoring/","title":"Live Monitoring","text":""},{"location":"live_monitoring/#1-monitor-gpu-metrics","title":"1. Monitor GPU Metrics","text":"<pre><code>Current GPU Metrics as of 2024-08-01 23:32:47:\n+------------------------------------+-------------------+---------------------------+-------------------+------------------------------------+\n|   GPU Index (Tesla V100-PCIE-32GB) |   Utilization (%) |   Power (W) / Max 250.0 W |   Temperature (C) |   Memory (MiB) / Total 32768.0 MiB |\n+====================================+===================+===========================+===================+====================================+\n|                                  0 |                60 |                    87.027 |                34 |                            2075.62 |\n+------------------------------------+-------------------+---------------------------+-------------------+------------------------------------+\n|                                  1 |                63 |                    87.318 |                40 |                            2075.62 |\n+------------------------------------+-------------------+---------------------------+-------------------+------------------------------------+\n</code></pre>"},{"location":"live_monitoring/#2-monitor-benchmark-container-logs","title":"2. Monitor Benchmark Container logs","text":"<pre><code>gpu_monitor --benchmark_image \"synthetic_regression\" --monitor_benchmark_logs\n\nContainer Logs:\n&lt;BEGIN&gt; Running benchmark synthetic_regression in training mode\n....&lt;BEGIN&gt; Parsing input arguments\n....&lt;ENDED&gt; Parsing input arguments [ELAPSED = 0.000035 sec]\n....&lt;BEGIN&gt; Creating dataset\n....&lt;ENDED&gt; Creating dataset [ELAPSED = 12.436691 sec]\n....&lt;MESSG&gt; Number of samples: 1024000\n....&lt;MESSG&gt; Total number of batches: 8000, 8000.0\n....&lt;BEGIN&gt; Training model\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n/root/anaconda3/envs/bench/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name | Type       | Params | Mode \n--------------------------------------------\n0 | net  | Sequential | 250 M  | train\n--------------------------------------------\n250 M     Trainable params\n0         Non-trainable params\n250 M     Total params\n1,000.404 Total estimated model params size (MB)\n11        Modules in train mode\n0         Modules in eval mode\n/root/anaconda3/envs/bench/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\nEpoch 0:   4%|\u258d         | 350/8000 [00:09&lt;03:23, 37.56it/s, v_num=0]\n</code></pre> <p>(Long container logs containing \"\\r\" to clear the line for progress bars are not a very efficiently processed, as it captures a snapshot of the entire container log at that moment. Potiential solution: use asynico package to capture and process the logs whilst the monitor is paused between intervals)</p>"},{"location":"live_monitoring/#3-save-png-timeseries-live","title":"3. Save png Timeseries Live","text":"<pre><code>gpu_monitor --benchmark_image \"synthetic_regression\" --plot_live\n</code></pre> <p>Gives you saves plot png during every reading so that the metrics can be viewed live.</p> <p>Previous Page | Next Page</p>"},{"location":"overview/","title":"Overview","text":"<p>The GPU Monitoring and Carbon Calculation Tool for Containerized Benchmarks is designed to monitor GPU performance and calculate associated carbon emissions during benchmarking tasks involving GPU operations.</p>"},{"location":"overview/#key-features","title":"Key Features","text":"<ul> <li> <p>Comprehensive GPU Monitoring: Utilizing the <code>GPUMonitor</code> class, the tool captures real-time GPU metrics such as utilization, power draw, temperature, and memory usage. These metrics are essential for evaluating the efficiency and performance of GPU-based applications.</p> </li> <li> <p>Carbon Emissions Calculation: The tool integrates with the National Grid ESO Regional Carbon Intensity API to estimate carbon emissions based on the energy consumed by the GPU during benchmarks. This feature is particularly useful for understanding the environmental impact of high-performance computing tasks.</p> </li> <li> <p>Export to VictoriaMetrics: Collected data can be optionally exported to VictoriaMetrics, allowing for long-term storage and analysis of GPU performance metrics. This is useful for tracking performance trends over time and integrating with other monitoring systems.</p> </li> <li> <p>Docker Integration: The tool is designed to work seamlessly with containerized environments. It includes scripts for building Docker images tailored to specific benchmarking tasks, ensuring that the monitoring tool can be easily integrated into existing workflows.</p> </li> <li> <p>Flexible Command-Line Interface: Users can customize the monitoring process with a variety of command-line arguments. These options allow for control over monitoring intervals, region-specific carbon calculations, live plotting of metrics, and more.</p> </li> <li> <p>Live Monitoring and Logging: The tool supports live monitoring of GPU metrics during benchmark execution, with the ability to log and display container logs in real-time. This feature is crucial for debugging and optimizing benchmark runs.</p> </li> </ul>"},{"location":"overview/#use-cases","title":"Use Cases","text":"<ul> <li> <p>Benchmarking GPU-Intensive Applications: Ideal for researchers, developers, and engineers who need to assess the performance of GPU-accelerated applications under various conditions.</p> </li> <li> <p>Environmental Impact Assessment: Provides a means to quantify the carbon emissions associated with GPU usage, which is important for organizations focused on sustainability and reducing their carbon footprint.</p> </li> <li> <p>Performance Optimization: By analyzing the collected GPU metrics, users can identify bottlenecks and optimize their applications to run more efficiently on GPUs.</p> </li> </ul> <p>Previous Page | Next Page</p>"},{"location":"work_to_do/","title":"Work To Do","text":"<ul> <li>Allow users to save container results</li> <li>Fix victoria_metrics exporter (username and password needed) and Test with grafana (add grafana link to docs)</li> <li>Improve live monitoring of container ie by threading</li> <li>Edit The way \"Carbon Forcast (gCO2/kWh)\" is computed so that the program checks the Forcast every 30 mins (or less) and computes an average at the end. (Another way to do this would be to multiply the energy consumed each 30 mins (or time interval) by the Forecast for that time and then add them together for a more accurate result. This way we could also give live power updates)</li> <li>using shell check from bash script (similar to pylint) on bash script</li> <li>Add CI tests for python scripts</li> </ul> <p>Previous Page </p>"}]}